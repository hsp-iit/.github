# Welcome to the Humanoid Sensing and Perception Github organization

The Humanoid Sensing and Perception is within Istituto Italiano di Tecnologia.

Our group studies algorithms and technologies that allow robots to sense the environment 
and react appropriately. Our strategy is to exploit the capability of robots to learn under 
human guidance or from the interaction with the environment by exploiting multiple sources 
of information (e.g. proprioception, vision, touch, and audition). We work on visual and 
tactile perception, for robot navigation and object manipulation. 
We also develop software tools and study software integration methodologies for
the development of complex behaviors.

Our platforms are the iCub and R1 humanoid robots, we focus on applications in the domain of
service robotics. 

Checkout [our IIT's website](https://www.iit.it/it/web/humanoid-sensing-and-perception).

Whenever possible we make code related to our research available to the community with open-source licenses. 

---
Bookmarks:

 - ‚öôÔ∏è [On-line detection and segmentation](https://github.com/hsp-iit/online-detection)
   - On-line Object Detection and Instance Segmentation project.
 - ‚öôÔ∏è [ROFT](https://github.com/hsp-iit/roft)
   - Real-time Optical Flow-aided 6D Object Pose and Velocity Tracking.
 - ‚öôÔ∏è [MASK-UKF](https://github.com/hsp-iit/mask-ukf)
   - Instance Segmentation Aided 6D Object Pose and Velocity Tracking using an Unscented Kalman Filter.
 - ‚öôÔ∏è [Fast-YCB Dataset](https://github.com/hsp-iit/fast-ycb)
   - An annotated dataset for 6D object tracking, with fast moving YCB objects. 
 - ‚öôÔ∏è [Digit simulator for gazebo](https://github.com/hsp-iit/gazebo-yarp-digit-plugin)
   - A tentative C++ wrapper for the Python based Digit tactile sensor simulation.
 - ‚öôÔ∏è [Tracking sliding objects with tactile feedback](https://github.com/hsp-iit/dekf-tactile-filtering)
   - A differentiable Extended Kalman Filter for object tracking under sliding regime
 - üìö [Visualization for grasp candidates](https://github.com/hsp-iit/manip-env-visu)
   - Barebones library to visualize simple manipulation environments
 - ‚öôÔ∏è [Robot environment for pybullet ](https://github.com/hsp-iit/pybullet-robot-envs)
   - A Python package that collects robotic environments based on the PyBullet simulator.
 - ‚öôÔ∏è [GRASPA Benchmark](https://github.com/hsp-iit/GRASPA-benchmark)
   - A grasping benchmark for comparing grasping planners across different robot platforms
 - ‚öôÔ∏è [YARP](https://github.com/robotology/yarp)
   - Yet Another Robot Platform - our middleware, check out official [documentation page](https://yarp.it)
 - ‚öôÔ∏è [YCM](https://github.com/robotology/ycm)
   - Extra CMake Modules for YARP and Friends, check out official [documentation page](http://robotology.github.io/ycm/gh-pages/git-master/index.html)
 - ‚öôÔ∏è  [visual-tracking-control](https://github.com/robotology/visual-tracking-control)
   - a suite of cross-platform applications for visual tracking and visual servoing for the humanoid robot platform iCub.
 - ‚öôÔ∏è  [navigation](https://github.com/robotology/navigation)
   - A collection of modules to perform 2D navigation with a YARP-based robot.
 - ‚öôÔ∏è [Cardinal points grasping](https://github.com/robotology/cardinal-points-grasp)
   - Simple superquadric-based grasping pose generator for iCub
 - ‚öôÔ∏è [Superquadric fitting](https://github.com/robotology/find-superquadric)
   - Solve an optimization problem to find out the best superquadric that fits a given partial point cloud.
 - ‚öôÔ∏è [On the Fly recognition](https://github.com/robotology/onthefly-recognition)
    - This demo allows to teach the iCub to visually recognize new objects "on the fly".
 - ‚öôÔ∏è https://github.com/robotology/himrep
    - This repository contains a collection of modules to extract features from images or to perform classification tasks on feature vectors
 - ‚öôÔ∏è https://github.com/robotology/r1-grasping
    - Grasping on R1 robot
 - ‚öôÔ∏è https://github.com/robotology/point-cloud-read
    - Acquire point clouds of specific objects in the scene in order to save or stream them.
 - ‚öôÔ∏è https://github.com/robotology/superquadric-grasp-demo
    - Object modeling and grasping with superquadrics and visual-servoing
 - ‚öôÔ∏è https://github.com/robotology/tactile-control
    - Improve grasp stability using tactile feedback.
 - üìö [superquadric-lib](https://github.com/robotology/superquadric-lib)
   - a Yarp-free library for computing and visualizing the superquadric representing an object and the relative grasping candidates for a generic robot.
 - üìö [superimpose-mesh-lib](https://github.com/robotology/superimpose-mesh-lib)
   - an augmented-reality library to superimpose 3D objects on a images.
 - üìö [bayes-filters-lib](https://github.com/robotology/bayes-filters-lib)
   - a recursive Bayesian estimation library.

---
Here is a list (autogenerated by [this](https://github.com/hsp-iit/.github/blob/main/.github/workflows/index-update.yml) github action) of ALL the public repos contained in this organization:
| Name | Description |  CI status | Docker |
| ----------- | ----------- | ----------- | ----------- |
| [2d_lidar_people_tracker](https://github.com/hsp-iit/2d_lidar_people_tracker) | |  | [![Build Status](https://github.com/hsp-iit/2d_lidar_people_tracker/workflows/Docker%20build/badge.svg)](https://github.com/hsp-iit/2d_lidar_people_tracker/actions?query=workflow%3A%22CI+Workflow%22) |
| [GRASPA-benchmark](https://github.com/hsp-iit/GRASPA-benchmark) | Repository gathering all code related to the paper GRASPA, Bottarel, Vezzani, Pattacini Natale, IEEE RA-L, vol. 5, no. 2, pp. 836-843, April 2020.)|  |  |
| [GRASPA-test](https://github.com/hsp-iit/GRASPA-test) | This repo contains the code for testing the GRASPA 1.0 on the iCub.|  |  |
| [HannesImitation](https://github.com/hsp-iit/HannesImitation) | HannesImitation is an imitation learning approach to control the Hannes prosthetic hand with a single Diffusion Policy to grasp several objects in diverse scenarios.|  |  |
| [KDPE](https://github.com/hsp-iit/KDPE) | This is the code for our work KDPE: A Kernel Density Estimation Strategy for Diffusion Policy Trajectory Selection, which is based on the original Diffusion Policy implementation.|  |  |
| [adaptive-tactile-force-control](https://github.com/hsp-iit/adaptive-tactile-force-control) | This repository contains the code associated to the paper "Adaptive Tactile Force Control in a Parallel Gripper with Low Positioning Resolution"|  |  |
| [behavior-stack-example](https://github.com/hsp-iit/behavior-stack-example) | |  |  |
| [bt_nav2_ergocub](https://github.com/hsp-iit/bt_nav2_ergocub) | Behavior Trees Nodes for Nav2 for the ergocub project|  |  |
| [byogg](https://github.com/hsp-iit/byogg) | [ICRA 2025] Bring Your Own Grasp Generator: Leveraging Robot Grasp Generation for Prosthetic Grasping|  |  |
| [concon-chi_benchmark](https://github.com/hsp-iit/concon-chi_benchmark) | Repository to host the code associated to the CVPR 2024 paper "ConCon-Chi: Concept-Context Chimera Benchmark for Personalized Vision-Language Tasks"|  |  |
| [convince_bts](https://github.com/hsp-iit/convince_bts) | |  |  |
| [dekf-tactile-filtering](https://github.com/hsp-iit/dekf-tactile-filtering) | A differentiable Extended Kalman Filter for object tracking under sliding regime|  |  |
| [dinoDet](https://github.com/hsp-iit/dinoDet) | [ICRA 2025] Continuous Wrist Control on the Hannes Prosthesis: a Vision-based Shared Autonomy Framework|  |  |
| [embodied-captioning](https://github.com/hsp-iit/embodied-captioning) | Official repository of the paper "Embodied Image Captioning: Self-supervised Learning Agents for Spatially Coherent Image Descriptions" accepted at ICCV 2025|  |  |
| [ergocub-behavior](https://github.com/hsp-iit/ergocub-behavior) | |  |  |
| [ergocub-bimanual](https://github.com/hsp-iit/ergocub-bimanual) | Two-hand control of the ergoCub robot.|  |  |
| [ergocub-cartesian-control](https://github.com/hsp-iit/ergocub-cartesian-control) | Libraries and programs to control ergoCub and R1 in cartesian space| [![Build Status](https://github.com/hsp-iit/ergocub-cartesian-control/workflows/CI%20Workflow/badge.svg)](https://github.com/hsp-iit/ergocub-cartesian-control/actions?query=workflow%3A%22CI+Workflow%22) |  |
| [ergocub-gaze-control](https://github.com/hsp-iit/ergocub-gaze-control) | |  |  |
| [ergocub-perception](https://github.com/hsp-iit/ergocub-perception) | |  |  |
| [ergocub-realsense-pose](https://github.com/hsp-iit/ergocub-realsense-pose) | |  |  |
| [ergocub-rpc-interfaces](https://github.com/hsp-iit/ergocub-rpc-interfaces) | This repository aim is to group the rpc interfaces exposed by the ergoCub modules|  |  |
| [ergocub_navigation](https://github.com/hsp-iit/ergocub_navigation) | ErgoCub Navigation Stack|  |  |
| [ergocub_suite](https://github.com/hsp-iit/ergocub_suite) | This repo contains all the sw dependencies and instructions needed by ergoCub robot|  |  |
| [fast-ycb](https://github.com/hsp-iit/fast-ycb) | The Fast-YCB Dataset|  |  |
| [gazebo-yarp-digit-plugin](https://github.com/hsp-iit/gazebo-yarp-digit-plugin) | A tentative C++ wrapper for the Python based Digit tactile sensor simulation|  |  |
| [handCamera](https://github.com/hsp-iit/handCamera) | |  |  |
| [hannes-wrist-control](https://github.com/hsp-iit/hannes-wrist-control) | [ICRA 2025] Continuous Wrist Control on the Hannes Prosthesis: a Vision-based Shared Autonomy Framework|  |  |
| [hemisphere-dataset-generation](https://github.com/hsp-iit/hemisphere-dataset-generation) | [ICRA 2025] Continuous Wrist Control on the Hannes Prosthesis: a Vision-based Shared Autonomy Framework|  |  |
| [hsp-land-annotation-tool](https://github.com/hsp-iit/hsp-land-annotation-tool) | |  |  |
| [icub-bimanual](https://github.com/hsp-iit/icub-bimanual) | Control classes for 2-handed control of the iCub robot|  |  |
| [learn_ltl](https://github.com/hsp-iit/learn_ltl) | Tool for passive learning of Linear Temporal Logic formulae|  |  |
| [manip-env-visu](https://github.com/hsp-iit/manip-env-visu) | Barebones library to visualize simple manipulation environments|  |  |
| [mask-ukf](https://github.com/hsp-iit/mask-ukf) | Instance Segmentation Aided 6D Object Pose and Velocity Tracking using an Unscented Kalman Filter|  |  |
| [masterThesisProject-Piquet](https://github.com/hsp-iit/masterThesisProject-Piquet) | |  |  |
| [multi-tactile-6d-estimation](https://github.com/hsp-iit/multi-tactile-6d-estimation) | Experiments for 6D estimation with tactile features.|  |  |
| [mutual-gaze-classifier-demo](https://github.com/hsp-iit/mutual-gaze-classifier-demo) | |  | [![Build Status](https://github.com/hsp-iit/mutual-gaze-classifier-demo/workflows/Docker%20build/badge.svg)](https://github.com/hsp-iit/mutual-gaze-classifier-demo/actions?query=workflow%3A%22CI+Workflow%22) |
| [mutual-gaze-detection](https://github.com/hsp-iit/mutual-gaze-detection) | |  |  |
| [online-attentive-object-detection](https://github.com/hsp-iit/online-attentive-object-detection) | |  |  |
| [online-detection](https://github.com/hsp-iit/online-detection) | This repository contains the python version of the source code for the experiments carried out for the On-line Object Detection and Instance Segmentation project.|  |  |
| [online-segmentation-demo](https://github.com/hsp-iit/online-segmentation-demo) | This repository contains the code used in the experiments on R1 and iCub in the papers Fast Object Segmentation Learning with Kernel-based Methods for Robotics and Learn Fast, Segment Well: Fast Object Segmentation Learning on the iCub Robot.|  |  |
| [prosthetic-grasping-experiments](https://github.com/hsp-iit/prosthetic-grasping-experiments) | [IROS 2022] Grasp Pre-shape Selection by Synthetic Training: Eye-in-hand Shared Control on the Hannes Prosthesis. Code to replicate the results in our paper.|  |  |
| [prosthetic-grasping-simulation](https://github.com/hsp-iit/prosthetic-grasping-simulation) | [IROS 2022] Grasp Pre-shape Selection by Synthetic Training: Eye-in-hand Shared Control on the Hannes Prosthesis. Code for synthetic data generation.|  |  |
| [pybullet-robot-envs](https://github.com/hsp-iit/pybullet-robot-envs) | A Python package that collects robotic environments based on the PyBullet simulator, suitable to develop and test Reinforcement Learning algorithms on simulated grasping and manipulation applications.|  |  |
| [r1-object-retrieval](https://github.com/hsp-iit/r1-object-retrieval) | |  | [![Build Status](https://github.com/hsp-iit/r1-object-retrieval/workflows/Docker%20build/badge.svg)](https://github.com/hsp-iit/r1-object-retrieval/actions?query=workflow%3A%22CI+Workflow%22) |
| [r1-steamdeck-launcher](https://github.com/hsp-iit/r1-steamdeck-launcher) | A repository to store all the scripts and files used to navigate r1 with the Steam Deck|  |  |
| [rl-icub-dexterous-manipulation](https://github.com/hsp-iit/rl-icub-dexterous-manipulation) | This repository contains the code to reproduce the experiments related to the Dexterous Manipulation with RL project on the iCub humanoid.|  |  |
| [roft](https://github.com/hsp-iit/roft) | Real-time Optical Flow-aided 6D Object Pose and Velocity Tracking|  |  |
| [roft-samples](https://github.com/hsp-iit/roft-samples) | A suite of applications based on ROFT|  |  |
| [sim2real-surface-classification](https://github.com/hsp-iit/sim2real-surface-classification) | |  |  |
| [tour-guide-robot](https://github.com/hsp-iit/tour-guide-robot) | A collection of modules and classes that can be used to perform guided tours with R1 robot or to simply interact with it. It's also the repo that contains the configuration files to perform autonomous navigation with R1|  | [![Build Status](https://github.com/hsp-iit/tour-guide-robot/workflows/Docker%20build/badge.svg)](https://github.com/hsp-iit/tour-guide-robot/actions?query=workflow%3A%22CI+Workflow%22) |
| [visual-servoing](https://github.com/hsp-iit/visual-servoing) | [ICRA 2025] Continuous Wrist Control on the Hannes Prosthesis: a Vision-based Shared Autonomy Framework|  |  |
